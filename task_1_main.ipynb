{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.1"
    },
    "colab": {
      "name": "task_1_main.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2TvyemfDlDmu"
      },
      "source": [
        "### Coursework coding instructions (please also see full coursework spec)\n",
        "\n",
        "Please choose if you want to do either Task 1 or Task 2. You should write your report about one task only.\n",
        "\n",
        "For the task you choose you will need to do two approaches:\n",
        "  - Approach 1, which can use use pre-trained embeddings / models\n",
        "  - Approach 2, which should not use any pre-trained embeddings or models\n",
        "We should be able to run both approaches from the same colab file\n",
        "\n",
        "#### Running your code:\n",
        "  - Your models should run automatically when running your colab file without further intervention\n",
        "  - For each task you should automatically output the performance of both models\n",
        "  - Your code should automatically download any libraries required\n",
        "\n",
        "#### Structure of your code:\n",
        "  - You are expected to use the 'train', 'eval' and 'model_performance' functions, although you may edit these as required\n",
        "  - Otherwise there are no restrictions on what you can do in your code\n",
        "\n",
        "#### Documentation:\n",
        "  - You are expected to produce a .README file summarising how you have approached both tasks\n",
        "\n",
        "#### Reproducibility:\n",
        "  - Your .README file should explain how to replicate the different experiments mentioned in your report\n",
        "\n",
        "Good luck! We are really looking forward to seeing your reports and your model code!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zny66AiKP-go",
        "outputId": "c2566884-417a-42a4-f815-53beac697588"
      },
      "source": [
        "# Data files are saved in the repository\n",
        "!git clone https://github.com/wang007123/NLP-CW1.git"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'NLP-CW1'...\n",
            "remote: Enumerating objects: 109, done.\u001b[K\n",
            "remote: Counting objects: 100% (109/109), done.\u001b[K\n",
            "remote: Compressing objects: 100% (105/105), done.\u001b[K\n",
            "remote: Total 109 (delta 29), reused 0 (delta 0), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (109/109), 652.08 KiB | 6.15 MiB/s, done.\n",
            "Resolving deltas: 100% (29/29), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WX9TqmK7lDoK"
      },
      "source": [
        "# Imports\n",
        "import torch\n",
        "import sys\n",
        "import spacy\n",
        "import math\n",
        "import copy\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "from torch import nn\n",
        "from torch.autograd import Variable\n",
        "from copy import deepcopy\n",
        "from sklearn.metrics import accuracy_score\n",
        "from torchtext import data\n",
        "from torchtext.vocab import Vectors\n",
        "\n",
        "\n",
        "# CW original import\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from torch.utils.data import Dataset, random_split\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LinearRegression\n",
        "import codecs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X09jt8VRlDoM"
      },
      "source": [
        "# Setting random seed and device\n",
        "SEED = 1\n",
        "\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed(SEED)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "\n",
        "use_cuda = torch.cuda.is_available()\n",
        "device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AqhlzLl6lDoO"
      },
      "source": [
        "# The paths of the data files in the repository\n",
        "\n",
        "train_file = '/content/NLP_CW1/Matrixposer/data/train.csv'\n",
        "test_file = '/content/NLP_CW1/Matrixposer/data/test.csv'\n",
        "val_file = '/content/NLP_CW1/Matrixposer/data/dev.csv'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3RCmF7xulDoP"
      },
      "source": [
        "# Number of epochs\n",
        "epochs = 40\n",
        "\n",
        "# Proportion of training data for train compared to dev\n",
        "train_proportion = 0.8"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u-bq4cjYR_cJ"
      },
      "source": [
        "# Other parameters\n",
        "class Config(object):\n",
        "    N = 2\n",
        "    d_model = 100\n",
        "    d_ff = 512\n",
        "    d_row = 60\n",
        "    dropout = 0.1\n",
        "    output_size = 1\n",
        "    lr = 0.013\n",
        "    batch_size = 64\n",
        "    max_sen_len = 60\n",
        "    pre_trained = True"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qAgZW6K1lDoR"
      },
      "source": [
        "# We define our training loop\n",
        "def train(train_iter, dev_iter, model, number_epoch):\n",
        "    \"\"\"\n",
        "    Training loop for the model, which calls on eval to evaluate after each epoch\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"Training model.\")\n",
        "    test_pre = None\n",
        "    for epoch in range(1, number_epoch+1):\n",
        "\n",
        "        model.train()\n",
        "        epoch_loss = 0\n",
        "        epoch_sse = 0\n",
        "        no_observations = 0\n",
        "\n",
        "\n",
        "        # Reduce learning rate as number of epochs increase\n",
        "        if (number_epoch == int(epochs / 3)) or (number_epoch == int(2 * epochs / 3)):\n",
        "            self.reduce_lr()\n",
        "\n",
        "        for batch in train_iter:\n",
        "            \n",
        "            feature, target = batch.text.to(device), (batch.label).to(device)\n",
        "\n",
        "            no_observations = no_observations + batch.label.shape[0]\n",
        "\n",
        "            predictions = model(feature)\n",
        "            \n",
        "            optimizer.zero_grad()\n",
        "            loss = criterion(predictions.view(-1), target)\n",
        "            sse, __ = model_performance(predictions.view(-1).detach().cpu().numpy(), target.detach().cpu().numpy())\n",
        "            \n",
        "            loss.backward()\n",
        "            \n",
        "            optimizer.step()\n",
        "\n",
        "            epoch_loss += loss.item()*target.shape[0]\n",
        "            epoch_sse += sse\n",
        "\n",
        "        valid_loss, valid_mse, pre, tar = eval(dev_iter, model)\n",
        "\n",
        "        test_loss, test_mse, test_pre, test_tar = eval(dataset.test_iterator, model)\n",
        "        \n",
        "        epoch_loss, epoch_mse = epoch_loss/ no_observations, epoch_sse / no_observations\n",
        "\n",
        "\n",
        "        print(f'| Epoch: {epoch:02} | Train Loss: {epoch_loss:.2f} |Train MSE: {epoch_mse:.2f} | Train RMSE: {(epoch_mse)**0.5:.2f} | \\\n",
        "         Val. Loss: {valid_loss:.2f} |Val. MSE: {valid_mse:.2f} |Val. RMSE: {valid_mse **0.5:.2f} | \\\n",
        "         Test. Loss: {test_loss:.2f} |Test. MSE: {test_mse:.2f} |Test. RMSE: {test_mse **0.5:.2f} |')\n",
        "    '''\n",
        "    #for hyper-parameter tuning\n",
        "    return number_epoch, lr, (epoch_mse)**0.5,epoch_loss, (valid_mse)**0.5,valid_loss, (test_mse)**0.5, test_loss,config.dropout\n",
        "    '''\n",
        "    return test_pre"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NzXeDgHmlDob"
      },
      "source": [
        "def eval(data_iter, model):\n",
        "    \"\"\"\n",
        "    Evaluating model performance on the dev set\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    epoch_loss = 0\n",
        "    epoch_sse = 0\n",
        "    pred_all = []\n",
        "    trg_all = []\n",
        "    no_observations = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in data_iter:\n",
        "            feature, target = batch.text, batch.label\n",
        "\n",
        "            feature, target = feature.to(device), target.to(device)\n",
        "            \n",
        "            no_observations = no_observations + target.shape[0]\n",
        "            predictions = model(feature)\n",
        "            \n",
        "            loss = criterion(predictions.view(-1), target)\n",
        "\n",
        "            pred, trg = predictions.view(-1).detach().cpu().numpy(), target.detach().cpu().numpy()\n",
        "            sse, __ = model_performance(pred, trg)\n",
        "\n",
        "            epoch_loss += loss.item()*batch.label.shape[0]\n",
        "            epoch_sse += sse\n",
        "            pred_all.extend(pred)\n",
        "            trg_all.extend(trg)\n",
        "            \n",
        "    return epoch_loss/no_observations, epoch_sse/no_observations, np.array(pred_all), np.array(trg_all)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2_22fHHElDog"
      },
      "source": [
        "# How we print the model performance\n",
        "def model_performance(output, target, print_output=False):\n",
        "    \"\"\"\n",
        "    Returns SSE and MSE per batch (printing the MSE and the RMSE)\n",
        "    \"\"\"\n",
        "\n",
        "    sq_error = (output - target)**2\n",
        "\n",
        "    sse = np.sum(sq_error)\n",
        "    mse = np.mean(sq_error)\n",
        "    rmse = np.sqrt(mse)\n",
        "\n",
        "    if print_output:\n",
        "        print(f'| MSE: {mse:.2f} | RMSE: {rmse:.2f} |')\n",
        "\n",
        "    return sse, mse"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DamNIXbnXdnr"
      },
      "source": [
        "class Matposer(nn.Module):\n",
        "    def __init__(self, config, src_vocab, pre_trained=False):\n",
        "        super(Matposer, self).__init__()\n",
        "        self.config = config\n",
        "\n",
        "        d_row, N, dropout = self.config.d_row, self.config.N, self.config.dropout\n",
        "        d_model, d_ff = self.config.d_model, self.config.d_ff\n",
        "\n",
        "        inter = Interactor(d_model, d_ff, out_row=d_row, dropout=dropout)\n",
        "        ff = PositionwiseFeedForward(d_model, d_ff, dropout)\n",
        "        position = PositionalEncoding(d_model, dropout)\n",
        "\n",
        "        self.encoder = Encoder(EncoderLayer(d_model, deepcopy(inter), deepcopy(ff), dropout), N)\n",
        "        self.src_embed = nn.Sequential(\n",
        "            Embeddings(d_model, src_vocab, pre_trained), deepcopy(position)\n",
        "        )\n",
        "        self.fc = nn.Linear(\n",
        "            d_model,\n",
        "            self.config.output_size\n",
        "        )\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        embedded_sents = self.src_embed(x.permute(1, 0)) \n",
        "        encoded_sents = self.encoder(embedded_sents)\n",
        "        final_feature_map = torch.sum(encoded_sents,1)\n",
        "        final_out = self.fc(final_feature_map)\n",
        "        return final_out\n",
        "\n",
        "    def reduce_lr(self):\n",
        "        print(\"Reducing LR\")\n",
        "        for g in self.optimizer.param_groups:\n",
        "            g['lr'] = g['lr'] / 2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qcc1kWE-XfhR"
      },
      "source": [
        "class Encoder(nn.Module):\n",
        "    '''\n",
        "    Matposer Encoder\n",
        "    It is a stack of N layers.\n",
        "    '''\n",
        "    def __init__(self, layer, N):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.layers = clones(layer, N)\n",
        "        self.norm = LayerNorm(layer.size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        for layer in self.layers:\n",
        "            x = layer(x)\n",
        "        return self.norm(x)\n",
        "\n",
        "\n",
        "class EncoderLayer(nn.Module):\n",
        "    '''\n",
        "    An encoder layer\n",
        "    Made up of Interactor and a feed forward layer\n",
        "    Each of these sublayers have residual and layer norm, implemented by SublayerOutput.\n",
        "    '''\n",
        "    def __init__(self, size, interactor, feed_forward, dropout):\n",
        "        super(EncoderLayer, self).__init__()\n",
        "        self.interactor = interactor\n",
        "        self.feed_forward = feed_forward\n",
        "        self.sublayer = SublayerOutput(size, dropout)\n",
        "        self.size = size\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"Matposer Encoder\"\n",
        "        x = self.interactor(x)\n",
        "        return self.sublayer(x, self.feed_forward)\n",
        "\n",
        "\n",
        "class PositionwiseFeedForward(nn.Module):\n",
        "    \"Positionwise feed-forward network.\"\n",
        "\n",
        "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
        "        super(PositionwiseFeedForward, self).__init__()\n",
        "        self.w_1 = nn.Linear(d_model, d_ff)\n",
        "        self.w_2 = nn.Linear(d_ff, d_model)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"Implements FFN equation.\"\n",
        "        return self.w_2(self.dropout(F.relu(self.w_1(x))))\n",
        "\n",
        "class Column_wise_nn(nn.Module):\n",
        "    def __init__(self, d_row, d_column, d_ff, dropout=None):\n",
        "        '''\n",
        "        initialize column-wise neural network\n",
        "        :param d_row: input row number\n",
        "        :param d_ff: middle size row number\n",
        "        :param dropout: default None\n",
        "        '''\n",
        "        super(Column_wise_nn, self).__init__()\n",
        "        self.w_1 = nn.Linear(d_row, d_ff)\n",
        "        self.w_2 = nn.Linear(d_ff, d_column)\n",
        "        if dropout is not None:\n",
        "            self.dropout = nn.Dropout(dropout)\n",
        "        else:\n",
        "            self.dropout = None\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.permute(0,2,1)\n",
        "        d_k = x.size(-1)\n",
        "        output = self.w_2(self.dropout(F.relu(self.w_1(x)))) / math.sqrt(d_k)\n",
        "        if self.dropout is not None:\n",
        "            output = self.dropout(output)\n",
        "        return output.permute(0,2,1)\n",
        "\n",
        "class Row_wise_nn(nn.Module):\n",
        "    def __init__(self, d_column, d_ff, out_row, dropout=None):\n",
        "        super(Row_wise_nn, self).__init__()\n",
        "        self.w_1 = nn.Linear(d_column, d_ff)\n",
        "        self.w_2 = nn.Linear(d_ff, out_row)\n",
        "        if dropout is not None:\n",
        "            self.dropout = nn.Dropout(dropout)\n",
        "        else:\n",
        "            self.dropout = None\n",
        "\n",
        "    def forward(self, x):\n",
        "        d_k = x.size(-1)\n",
        "        output = self.w_2(self.dropout(F.relu(self.w_1(x)))) / math.sqrt(d_k)\n",
        "        output = F.softmax(output, dim=-1)\n",
        "        if self.dropout is not None:\n",
        "            output = self.dropout(output)\n",
        "\n",
        "        return output\n",
        "\n",
        "\n",
        "class Interactor(nn.Module):\n",
        "    def __init__(self, d_column, d_ff, out_row=30, dropout=0.1):\n",
        "        '''\n",
        "        :param d_row: dimension of output row number\n",
        "        :param d_column: dimension of input column number\n",
        "        :param d_ff: dimension of middle neural\n",
        "        :param dropout: default 0.1\n",
        "        '''\n",
        "        super(Interactor, self).__init__()\n",
        "        self.column_wise_nn = Column_wise_nn(out_row, d_column, d_ff, dropout)\n",
        "        self.row_wise_nn = Row_wise_nn(d_column, d_ff, out_row, dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        left_transposer = self.row_wise_nn(x)\n",
        "        middle_term = torch.matmul(left_transposer.permute(0,2,1), x)\n",
        "        output = self.column_wise_nn(middle_term)\n",
        "        return output\n",
        "\n",
        "\n",
        "class LayerNorm(nn.Module):\n",
        "    \"Construct a layer normalization module.\"\n",
        "\n",
        "    def __init__(self, features, eps=1e-6):\n",
        "        super(LayerNorm, self).__init__()\n",
        "        self.a_2 = nn.Parameter(torch.ones(features))\n",
        "        self.b_2 = nn.Parameter(torch.zeros(features))\n",
        "        self.eps = eps\n",
        "\n",
        "    def forward(self, x):\n",
        "        mean = x.mean(-1, keepdim=True)\n",
        "        std = x.std(-1, keepdim=True)\n",
        "        return self.a_2 * (x - mean) / (std + self.eps) + self.b_2\n",
        "        \n",
        "        \n",
        "class SublayerOutput(nn.Module):\n",
        "    '''\n",
        "    A residual connection followed by a layer norm\n",
        "    '''\n",
        "\n",
        "    def __init__(self, size, dropout):\n",
        "        super(SublayerOutput, self).__init__()\n",
        "        self.norm = LayerNorm(size)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, sublayer):\n",
        "        \"Apply residual connection to any sublayer with the same size.\"\n",
        "        return x + self.dropout(sublayer(self.norm(x)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eMTvrWovYF23"
      },
      "source": [
        "class Embeddings(nn.Module):\n",
        "    '''\n",
        "    Usual Embedding layer with weights multiplied by sqrt(d_model)\n",
        "    '''\n",
        "\n",
        "    def __init__(self, d_model, vocab, pre_trained=False):\n",
        "        super(Embeddings, self).__init__()\n",
        "        if pre_trained is False:\n",
        "            self.lut = nn.Embedding(len(vocab), d_model)\n",
        "        else:\n",
        "            self.lut = nn.Embedding.from_pretrained(vocab)\n",
        "        self.d_model = d_model\n",
        "\n",
        "        # self.d_model = d_model\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.lut(x) * math.sqrt(self.d_model)\n",
        "\n",
        "\n",
        "class PositionalEncoding(nn.Module):\n",
        "    \"Implement the PE function\"\n",
        "\n",
        "    def __init__(self, d_model, dropout, max_len=5000):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "        # Compute the positional encodings once in log space.\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() *\n",
        "                             -(math.log(10000.0) / d_model))\n",
        "        pe[:, 0::2] = torch.sin(torch.as_tensor(position.numpy() * div_term.unsqueeze(0).numpy()))\n",
        "        pe[:, 1::2] = torch.cos(\n",
        "            torch.as_tensor(position.numpy() * div_term.unsqueeze(0).numpy()))  # torch.cos(position * div_term)\n",
        "        pe = pe.unsqueeze(0)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + Variable(self.pe[:, :x.size(1)],\n",
        "                         requires_grad=False)\n",
        "        return self.dropout(x)\n",
        "\n",
        "\n",
        "def get_embedding_matrix(vocab_chars):\n",
        "    # return one hot emdding\n",
        "    vocabulary_size = len(vocab_chars)\n",
        "    onehot_matrix = np.eye(vocabulary_size, vocabulary_size)\n",
        "    return onehot_matrix\n",
        "\n",
        "def clones(module, N):\n",
        "    \"Produce N identical layers\"\n",
        "    return nn.ModuleList([copy.deepcopy(module) for _ in range(N)])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7X360GGYYPDX"
      },
      "source": [
        "class Dataset(object):\n",
        "    def __init__(self, config):\n",
        "        self.config = config\n",
        "        self.train_iterator = None\n",
        "        self.test_iterator = None\n",
        "        self.val_iterator = None\n",
        "        self.vocab = []\n",
        "        self.word_embeddings = {}\n",
        "\n",
        "    def load_data(self, train_file, test_file, val_file=None, pre_trained=False):\n",
        "        '''\n",
        "        Loads the data from files\n",
        "        Sets up iterators for training, validation and test data\n",
        "        Also create vocabulary and word embeddings based on the data\n",
        "        Inputs:\n",
        "            train_file (String): absolute path to training file\n",
        "            test_file (String): absolute path to test file\n",
        "            val_file (String): absolute path to validation file\n",
        "        '''\n",
        "        # Loading Tokenizer\n",
        "        NLP = spacy.load('en')\n",
        "\n",
        "        def tokenizer(sent):\n",
        "            return list(\n",
        "                x.text for x in NLP.tokenizer(sent) if x.text != \" \")\n",
        "\n",
        "        # Creating Filed for data\n",
        "        TEXT = data.Field(sequential=True, tokenize=tokenizer, lower=True, fix_length=self.config.max_sen_len)\n",
        "        LABEL = data.Field(sequential=False, use_vocab=False, dtype=torch.float)\n",
        "        \n",
        "        datafields = [(\"text\", TEXT), (\"label\", LABEL)]\n",
        "\n",
        "        # Load data from csv into torchtext.data.Dataset\n",
        "        # train\n",
        "        train_df = pd.read_csv(train_file)\n",
        "\n",
        "        train_df = train_df[['original', 'meanGrade']]\n",
        "        train_df = train_df.rename(columns={'original': \"text\", 'meanGrade': 'label'})\n",
        "        train_examples = [\n",
        "            data.Example.fromlist(i, datafields) for i in train_df.values.tolist()]\n",
        "        train_data = data.Dataset(train_examples, datafields)\n",
        "        \n",
        "        # test\n",
        "        test_df = pd.read_csv(test_file)\n",
        "\n",
        "        test_df = test_df[['original', 'meanGrade']]\n",
        "        test_df = test_df.rename(columns={'original': \"text\", 'meanGrade': 'label'})\n",
        "        \n",
        "        test_examples = [\n",
        "            data.Example.fromlist(\n",
        "                i, datafields) for i in test_df.values.tolist()]\n",
        "        test_data = data.Dataset(test_examples, datafields)\n",
        "\n",
        "        # If validation file exists, load it. Otherwise get validation data\n",
        "        # from training data\n",
        "        if val_file:\n",
        "           \n",
        "            val_df = pd.read_csv(val_file)\n",
        "            val_df = val_df[['original', 'meanGrade']]\n",
        "            val_df = val_df.rename(columns={'original': \"text\",'meanGrade': 'label'})\n",
        "            \n",
        "            val_examples = [\n",
        "                data.Example.fromlist(\n",
        "                    i, datafields) for i in val_df.values.tolist()]\n",
        "            val_data = data.Dataset(val_examples, datafields)\n",
        "        else:\n",
        "            train_data, val_data = train_data.split(split_ratio=0.8)\n",
        "\n",
        "        if pre_trained:\n",
        "            TEXT.build_vocab(train_data, vectors='glove.6B.100d')\n",
        "            self.vocab = TEXT.vocab.vectors\n",
        "        else:\n",
        "            TEXT.build_vocab(train_data)\n",
        "            self.vocab = TEXT.vocab\n",
        "\n",
        "        self.train_iterator = data.BucketIterator(\n",
        "            (train_data),\n",
        "            batch_size=self.config.batch_size,\n",
        "            sort_key=lambda x: len(x.text),\n",
        "            repeat=False,\n",
        "            shuffle=True\n",
        "        )\n",
        "        self.val_iterator = data.BucketIterator(\n",
        "            (val_data),\n",
        "            batch_size=self.config.batch_size,\n",
        "            sort_key=lambda x: len(x.text),\n",
        "            repeat=False,\n",
        "            shuffle=False\n",
        "        )\n",
        "        self.test_iterator = data.BucketIterator(\n",
        "            (test_data),\n",
        "            batch_size=self.config.batch_size,\n",
        "            sort_key=lambda x: len(x.text),\n",
        "            repeat=False,\n",
        "            shuffle=False\n",
        "        )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qog1HIpVqe6a"
      },
      "source": [
        "#### Approach ONE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "bp9d32sOlDo6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "60705834-48d3-4b2b-bd55-da1e76dc3fcc"
      },
      "source": [
        "# Approach ONE\n",
        "torch.cuda.empty_cache()\n",
        "config = Config\n",
        "config.pre_trained = True\n",
        "\n",
        "dataset = Dataset(config)\n",
        "dataset.load_data(train_file, test_file, val_file, config.pre_trained)\n",
        "\n",
        "    \n",
        "model = Matposer(config, dataset.vocab, config.pre_trained)\n",
        "model.to(device)\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr=config.lr)\n",
        "\n",
        "criterion = nn.MSELoss()\n",
        "criterion = criterion.to(device)\n",
        "predicted = train(dataset.train_iterator, dataset.val_iterator,model, epochs)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training model.\n",
            "| Epoch: 01 | Train Loss: 1109.61 |Train MSE: 1109.61 | Train RMSE: 33.31 |          Val. Loss: 0.50 |Val. MSE: 0.50 |Val. RMSE: 0.71 |          Test. Loss: 0.50 |Test. MSE: 0.50 |Test. RMSE: 0.71 |\n",
            "| Epoch: 02 | Train Loss: 2.54 |Train MSE: 2.54 | Train RMSE: 1.59 |          Val. Loss: 0.38 |Val. MSE: 0.38 |Val. RMSE: 0.61 |          Test. Loss: 0.38 |Test. MSE: 0.38 |Test. RMSE: 0.61 |\n",
            "| Epoch: 03 | Train Loss: 2.27 |Train MSE: 2.27 | Train RMSE: 1.51 |          Val. Loss: 0.36 |Val. MSE: 0.36 |Val. RMSE: 0.60 |          Test. Loss: 0.36 |Test. MSE: 0.36 |Test. RMSE: 0.60 |\n",
            "| Epoch: 04 | Train Loss: 2.06 |Train MSE: 2.06 | Train RMSE: 1.43 |          Val. Loss: 0.38 |Val. MSE: 0.38 |Val. RMSE: 0.62 |          Test. Loss: 0.38 |Test. MSE: 0.38 |Test. RMSE: 0.62 |\n",
            "| Epoch: 05 | Train Loss: 1.93 |Train MSE: 1.93 | Train RMSE: 1.39 |          Val. Loss: 1.03 |Val. MSE: 1.03 |Val. RMSE: 1.01 |          Test. Loss: 1.04 |Test. MSE: 1.04 |Test. RMSE: 1.02 |\n",
            "| Epoch: 06 | Train Loss: 1.72 |Train MSE: 1.72 | Train RMSE: 1.31 |          Val. Loss: 0.47 |Val. MSE: 0.47 |Val. RMSE: 0.68 |          Test. Loss: 0.47 |Test. MSE: 0.47 |Test. RMSE: 0.68 |\n",
            "| Epoch: 07 | Train Loss: 1.55 |Train MSE: 1.55 | Train RMSE: 1.24 |          Val. Loss: 0.40 |Val. MSE: 0.40 |Val. RMSE: 0.63 |          Test. Loss: 0.40 |Test. MSE: 0.40 |Test. RMSE: 0.63 |\n",
            "| Epoch: 08 | Train Loss: 1.38 |Train MSE: 1.38 | Train RMSE: 1.17 |          Val. Loss: 0.40 |Val. MSE: 0.40 |Val. RMSE: 0.64 |          Test. Loss: 0.41 |Test. MSE: 0.41 |Test. RMSE: 0.64 |\n",
            "| Epoch: 09 | Train Loss: 1.29 |Train MSE: 1.29 | Train RMSE: 1.14 |          Val. Loss: 0.37 |Val. MSE: 0.37 |Val. RMSE: 0.61 |          Test. Loss: 0.37 |Test. MSE: 0.37 |Test. RMSE: 0.61 |\n",
            "| Epoch: 10 | Train Loss: 1.16 |Train MSE: 1.16 | Train RMSE: 1.08 |          Val. Loss: 0.71 |Val. MSE: 0.71 |Val. RMSE: 0.84 |          Test. Loss: 0.72 |Test. MSE: 0.72 |Test. RMSE: 0.85 |\n",
            "| Epoch: 11 | Train Loss: 1.05 |Train MSE: 1.05 | Train RMSE: 1.02 |          Val. Loss: 0.39 |Val. MSE: 0.39 |Val. RMSE: 0.63 |          Test. Loss: 0.39 |Test. MSE: 0.39 |Test. RMSE: 0.63 |\n",
            "| Epoch: 12 | Train Loss: 0.95 |Train MSE: 0.95 | Train RMSE: 0.97 |          Val. Loss: 0.34 |Val. MSE: 0.34 |Val. RMSE: 0.59 |          Test. Loss: 0.34 |Test. MSE: 0.34 |Test. RMSE: 0.59 |\n",
            "| Epoch: 13 | Train Loss: 0.86 |Train MSE: 0.86 | Train RMSE: 0.93 |          Val. Loss: 0.53 |Val. MSE: 0.53 |Val. RMSE: 0.73 |          Test. Loss: 0.53 |Test. MSE: 0.53 |Test. RMSE: 0.73 |\n",
            "| Epoch: 14 | Train Loss: 0.78 |Train MSE: 0.78 | Train RMSE: 0.88 |          Val. Loss: 0.40 |Val. MSE: 0.40 |Val. RMSE: 0.63 |          Test. Loss: 0.40 |Test. MSE: 0.40 |Test. RMSE: 0.63 |\n",
            "| Epoch: 15 | Train Loss: 0.70 |Train MSE: 0.70 | Train RMSE: 0.83 |          Val. Loss: 0.35 |Val. MSE: 0.35 |Val. RMSE: 0.59 |          Test. Loss: 0.35 |Test. MSE: 0.35 |Test. RMSE: 0.59 |\n",
            "| Epoch: 16 | Train Loss: 0.66 |Train MSE: 0.66 | Train RMSE: 0.81 |          Val. Loss: 0.42 |Val. MSE: 0.42 |Val. RMSE: 0.65 |          Test. Loss: 0.42 |Test. MSE: 0.42 |Test. RMSE: 0.65 |\n",
            "| Epoch: 17 | Train Loss: 0.59 |Train MSE: 0.59 | Train RMSE: 0.77 |          Val. Loss: 0.65 |Val. MSE: 0.65 |Val. RMSE: 0.81 |          Test. Loss: 0.65 |Test. MSE: 0.65 |Test. RMSE: 0.81 |\n",
            "| Epoch: 18 | Train Loss: 0.56 |Train MSE: 0.56 | Train RMSE: 0.75 |          Val. Loss: 0.37 |Val. MSE: 0.37 |Val. RMSE: 0.61 |          Test. Loss: 0.37 |Test. MSE: 0.37 |Test. RMSE: 0.61 |\n",
            "| Epoch: 19 | Train Loss: 0.52 |Train MSE: 0.52 | Train RMSE: 0.72 |          Val. Loss: 0.39 |Val. MSE: 0.39 |Val. RMSE: 0.63 |          Test. Loss: 0.39 |Test. MSE: 0.39 |Test. RMSE: 0.63 |\n",
            "| Epoch: 20 | Train Loss: 0.47 |Train MSE: 0.47 | Train RMSE: 0.69 |          Val. Loss: 0.59 |Val. MSE: 0.59 |Val. RMSE: 0.77 |          Test. Loss: 0.59 |Test. MSE: 0.59 |Test. RMSE: 0.77 |\n",
            "| Epoch: 21 | Train Loss: 0.46 |Train MSE: 0.46 | Train RMSE: 0.68 |          Val. Loss: 0.36 |Val. MSE: 0.36 |Val. RMSE: 0.60 |          Test. Loss: 0.36 |Test. MSE: 0.36 |Test. RMSE: 0.60 |\n",
            "| Epoch: 22 | Train Loss: 0.43 |Train MSE: 0.43 | Train RMSE: 0.66 |          Val. Loss: 0.33 |Val. MSE: 0.33 |Val. RMSE: 0.58 |          Test. Loss: 0.33 |Test. MSE: 0.33 |Test. RMSE: 0.57 |\n",
            "| Epoch: 23 | Train Loss: 0.41 |Train MSE: 0.41 | Train RMSE: 0.64 |          Val. Loss: 0.38 |Val. MSE: 0.38 |Val. RMSE: 0.61 |          Test. Loss: 0.38 |Test. MSE: 0.38 |Test. RMSE: 0.61 |\n",
            "| Epoch: 24 | Train Loss: 0.40 |Train MSE: 0.40 | Train RMSE: 0.63 |          Val. Loss: 0.34 |Val. MSE: 0.34 |Val. RMSE: 0.58 |          Test. Loss: 0.34 |Test. MSE: 0.34 |Test. RMSE: 0.58 |\n",
            "| Epoch: 25 | Train Loss: 0.40 |Train MSE: 0.40 | Train RMSE: 0.63 |          Val. Loss: 0.35 |Val. MSE: 0.35 |Val. RMSE: 0.59 |          Test. Loss: 0.35 |Test. MSE: 0.35 |Test. RMSE: 0.59 |\n",
            "| Epoch: 26 | Train Loss: 0.38 |Train MSE: 0.38 | Train RMSE: 0.61 |          Val. Loss: 0.43 |Val. MSE: 0.43 |Val. RMSE: 0.66 |          Test. Loss: 0.43 |Test. MSE: 0.43 |Test. RMSE: 0.66 |\n",
            "| Epoch: 27 | Train Loss: 0.39 |Train MSE: 0.39 | Train RMSE: 0.62 |          Val. Loss: 0.47 |Val. MSE: 0.47 |Val. RMSE: 0.69 |          Test. Loss: 0.47 |Test. MSE: 0.47 |Test. RMSE: 0.69 |\n",
            "| Epoch: 28 | Train Loss: 0.38 |Train MSE: 0.38 | Train RMSE: 0.62 |          Val. Loss: 0.34 |Val. MSE: 0.34 |Val. RMSE: 0.58 |          Test. Loss: 0.33 |Test. MSE: 0.33 |Test. RMSE: 0.58 |\n",
            "| Epoch: 29 | Train Loss: 0.38 |Train MSE: 0.38 | Train RMSE: 0.61 |          Val. Loss: 0.45 |Val. MSE: 0.45 |Val. RMSE: 0.67 |          Test. Loss: 0.45 |Test. MSE: 0.45 |Test. RMSE: 0.67 |\n",
            "| Epoch: 30 | Train Loss: 0.37 |Train MSE: 0.37 | Train RMSE: 0.61 |          Val. Loss: 0.36 |Val. MSE: 0.36 |Val. RMSE: 0.60 |          Test. Loss: 0.35 |Test. MSE: 0.35 |Test. RMSE: 0.59 |\n",
            "| Epoch: 31 | Train Loss: 0.39 |Train MSE: 0.39 | Train RMSE: 0.63 |          Val. Loss: 0.36 |Val. MSE: 0.36 |Val. RMSE: 0.60 |          Test. Loss: 0.35 |Test. MSE: 0.35 |Test. RMSE: 0.59 |\n",
            "| Epoch: 32 | Train Loss: 0.40 |Train MSE: 0.40 | Train RMSE: 0.63 |          Val. Loss: 0.34 |Val. MSE: 0.34 |Val. RMSE: 0.58 |          Test. Loss: 0.33 |Test. MSE: 0.33 |Test. RMSE: 0.57 |\n",
            "| Epoch: 33 | Train Loss: 0.39 |Train MSE: 0.39 | Train RMSE: 0.63 |          Val. Loss: 0.68 |Val. MSE: 0.68 |Val. RMSE: 0.82 |          Test. Loss: 0.68 |Test. MSE: 0.68 |Test. RMSE: 0.82 |\n",
            "| Epoch: 34 | Train Loss: 0.36 |Train MSE: 0.36 | Train RMSE: 0.60 |          Val. Loss: 0.36 |Val. MSE: 0.36 |Val. RMSE: 0.60 |          Test. Loss: 0.35 |Test. MSE: 0.35 |Test. RMSE: 0.59 |\n",
            "| Epoch: 35 | Train Loss: 0.41 |Train MSE: 0.41 | Train RMSE: 0.64 |          Val. Loss: 0.33 |Val. MSE: 0.33 |Val. RMSE: 0.58 |          Test. Loss: 0.33 |Test. MSE: 0.33 |Test. RMSE: 0.57 |\n",
            "| Epoch: 36 | Train Loss: 0.37 |Train MSE: 0.37 | Train RMSE: 0.61 |          Val. Loss: 0.35 |Val. MSE: 0.35 |Val. RMSE: 0.60 |          Test. Loss: 0.35 |Test. MSE: 0.35 |Test. RMSE: 0.59 |\n",
            "| Epoch: 37 | Train Loss: 0.40 |Train MSE: 0.40 | Train RMSE: 0.63 |          Val. Loss: 0.75 |Val. MSE: 0.75 |Val. RMSE: 0.87 |          Test. Loss: 0.75 |Test. MSE: 0.75 |Test. RMSE: 0.87 |\n",
            "| Epoch: 38 | Train Loss: 0.38 |Train MSE: 0.38 | Train RMSE: 0.62 |          Val. Loss: 0.38 |Val. MSE: 0.38 |Val. RMSE: 0.62 |          Test. Loss: 0.37 |Test. MSE: 0.37 |Test. RMSE: 0.61 |\n",
            "| Epoch: 39 | Train Loss: 0.38 |Train MSE: 0.38 | Train RMSE: 0.62 |          Val. Loss: 0.71 |Val. MSE: 0.71 |Val. RMSE: 0.84 |          Test. Loss: 0.71 |Test. MSE: 0.71 |Test. RMSE: 0.84 |\n",
            "| Epoch: 40 | Train Loss: 0.38 |Train MSE: 0.38 | Train RMSE: 0.62 |          Val. Loss: 0.35 |Val. MSE: 0.35 |Val. RMSE: 0.59 |          Test. Loss: 0.34 |Test. MSE: 0.34 |Test. RMSE: 0.58 |\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rdeFaoc3lDpK"
      },
      "source": [
        "#### Approach 2: No pre-trained representations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "46gm47T4lDpQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "20d309af-ca77-4911-d102-28932bf87f03"
      },
      "source": [
        "# Load data\n",
        "train_df = pd.read_csv(train_file)\n",
        "test_df = pd.read_csv(test_file)\n",
        "train_and_dev = train_df['edit']\n",
        "\n",
        "training_data, dev_data, training_y, dev_y = train_test_split(train_df['edit'], train_df['meanGrade'],\n",
        "                                                                        test_size=(1-train_proportion),\n",
        "                                                                        random_state=42)\n",
        "\n",
        "# We train a Tf-idf model\n",
        "count_vect = CountVectorizer(stop_words='english')\n",
        "train_counts = count_vect.fit_transform(training_data)\n",
        "transformer = TfidfTransformer().fit(train_counts)\n",
        "train_counts = transformer.transform(train_counts)\n",
        "regression_model = LinearRegression().fit(train_counts, training_y)\n",
        "\n",
        "# Train predictions\n",
        "predicted_train = regression_model.predict(train_counts)\n",
        "\n",
        "# Calculate Tf-idf using train and dev, and validate model on dev:\n",
        "test_and_test_counts = count_vect.transform(train_and_dev)\n",
        "transformer = TfidfTransformer().fit(test_and_test_counts)\n",
        "\n",
        "test_counts = count_vect.transform(dev_data)\n",
        "\n",
        "test_counts = transformer.transform(test_counts)\n",
        "\n",
        "# Dev predictions\n",
        "predicted = regression_model.predict(test_counts)\n",
        "\n",
        "# We run the evaluation:\n",
        "print(\"\\nTrain performance:\")\n",
        "sse, mse = model_performance(predicted_train, training_y, True)\n",
        "\n",
        "print(\"\\nDev performance:\")\n",
        "sse, mse = model_performance(predicted, dev_y, True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train performance:\n",
            "| MSE: 0.13 | RMSE: 0.37 |\n",
            "\n",
            "Dev performance:\n",
            "| MSE: 0.36 | RMSE: 0.60 |\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9HyHwHkUlDpa"
      },
      "source": [
        "#### Baseline for task 2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7DA3q4o1lDpd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f25c2f2d-5ead-443e-b1fb-5e0069d9afd3"
      },
      "source": [
        "#Â Baseline for the task\n",
        "pred_baseline = torch.zeros(len(dev_y)) + np.mean(training_y)\n",
        "print(\"\\nBaseline performance:\")\n",
        "sse, mse = model_performance(pred_baseline, dev_y, True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Baseline performance:\n",
            "| MSE: 0.34 | RMSE: 0.58 |\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uFfUC9D9qSau"
      },
      "source": [
        "#### Approach TWO"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "84nQDZyBlDpg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8434bdc2-0eca-46fe-b76d-cd4737320b45"
      },
      "source": [
        "config.pre_trained = False\n",
        "config.lr = 0.0001\n",
        "config.dropout = 0.1\n",
        "model = Matposer(config, dataset.vocab, config.pre_trained)\n",
        "model.to(device)\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr=config.lr)\n",
        "\n",
        "criterion = nn.MSELoss()\n",
        "criterion = criterion.to(device)\n",
        "predicted = train(dataset.train_iterator, dataset.val_iterator,model, epochs)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training model.\n",
            "| Epoch: 01 | Train Loss: 36.38 |Train MSE: 36.38 | Train RMSE: 6.03 |          Val. Loss: 10.82 |Val. MSE: 10.82 |Val. RMSE: 3.29 |          Test. Loss: 10.75 |Test. MSE: 10.75 |Test. RMSE: 3.28 |\n",
            "| Epoch: 02 | Train Loss: 7.55 |Train MSE: 7.55 | Train RMSE: 2.75 |          Val. Loss: 1.50 |Val. MSE: 1.50 |Val. RMSE: 1.22 |          Test. Loss: 1.48 |Test. MSE: 1.48 |Test. RMSE: 1.22 |\n",
            "| Epoch: 03 | Train Loss: 5.14 |Train MSE: 5.14 | Train RMSE: 2.27 |          Val. Loss: 0.80 |Val. MSE: 0.80 |Val. RMSE: 0.89 |          Test. Loss: 0.78 |Test. MSE: 0.78 |Test. RMSE: 0.88 |\n",
            "| Epoch: 04 | Train Loss: 3.59 |Train MSE: 3.59 | Train RMSE: 1.89 |          Val. Loss: 1.38 |Val. MSE: 1.38 |Val. RMSE: 1.17 |          Test. Loss: 1.34 |Test. MSE: 1.34 |Test. RMSE: 1.16 |\n",
            "| Epoch: 05 | Train Loss: 2.73 |Train MSE: 2.73 | Train RMSE: 1.65 |          Val. Loss: 1.18 |Val. MSE: 1.18 |Val. RMSE: 1.09 |          Test. Loss: 1.14 |Test. MSE: 1.14 |Test. RMSE: 1.07 |\n",
            "| Epoch: 06 | Train Loss: 2.12 |Train MSE: 2.12 | Train RMSE: 1.46 |          Val. Loss: 0.37 |Val. MSE: 0.37 |Val. RMSE: 0.61 |          Test. Loss: 0.37 |Test. MSE: 0.37 |Test. RMSE: 0.60 |\n",
            "| Epoch: 07 | Train Loss: 1.80 |Train MSE: 1.80 | Train RMSE: 1.34 |          Val. Loss: 0.35 |Val. MSE: 0.35 |Val. RMSE: 0.59 |          Test. Loss: 0.35 |Test. MSE: 0.35 |Test. RMSE: 0.59 |\n",
            "| Epoch: 08 | Train Loss: 1.55 |Train MSE: 1.55 | Train RMSE: 1.25 |          Val. Loss: 0.50 |Val. MSE: 0.50 |Val. RMSE: 0.71 |          Test. Loss: 0.48 |Test. MSE: 0.48 |Test. RMSE: 0.69 |\n",
            "| Epoch: 09 | Train Loss: 1.39 |Train MSE: 1.39 | Train RMSE: 1.18 |          Val. Loss: 0.35 |Val. MSE: 0.35 |Val. RMSE: 0.59 |          Test. Loss: 0.34 |Test. MSE: 0.34 |Test. RMSE: 0.59 |\n",
            "| Epoch: 10 | Train Loss: 1.20 |Train MSE: 1.20 | Train RMSE: 1.10 |          Val. Loss: 0.45 |Val. MSE: 0.45 |Val. RMSE: 0.67 |          Test. Loss: 0.43 |Test. MSE: 0.43 |Test. RMSE: 0.66 |\n",
            "| Epoch: 11 | Train Loss: 1.12 |Train MSE: 1.12 | Train RMSE: 1.06 |          Val. Loss: 0.35 |Val. MSE: 0.35 |Val. RMSE: 0.60 |          Test. Loss: 0.34 |Test. MSE: 0.34 |Test. RMSE: 0.59 |\n",
            "| Epoch: 12 | Train Loss: 1.02 |Train MSE: 1.02 | Train RMSE: 1.01 |          Val. Loss: 0.53 |Val. MSE: 0.53 |Val. RMSE: 0.73 |          Test. Loss: 0.53 |Test. MSE: 0.53 |Test. RMSE: 0.73 |\n",
            "| Epoch: 13 | Train Loss: 0.94 |Train MSE: 0.94 | Train RMSE: 0.97 |          Val. Loss: 0.49 |Val. MSE: 0.49 |Val. RMSE: 0.70 |          Test. Loss: 0.49 |Test. MSE: 0.49 |Test. RMSE: 0.70 |\n",
            "| Epoch: 14 | Train Loss: 0.87 |Train MSE: 0.87 | Train RMSE: 0.93 |          Val. Loss: 0.36 |Val. MSE: 0.36 |Val. RMSE: 0.60 |          Test. Loss: 0.35 |Test. MSE: 0.35 |Test. RMSE: 0.59 |\n",
            "| Epoch: 15 | Train Loss: 0.84 |Train MSE: 0.84 | Train RMSE: 0.92 |          Val. Loss: 0.86 |Val. MSE: 0.86 |Val. RMSE: 0.92 |          Test. Loss: 0.87 |Test. MSE: 0.87 |Test. RMSE: 0.93 |\n",
            "| Epoch: 16 | Train Loss: 0.78 |Train MSE: 0.78 | Train RMSE: 0.88 |          Val. Loss: 0.36 |Val. MSE: 0.36 |Val. RMSE: 0.60 |          Test. Loss: 0.35 |Test. MSE: 0.35 |Test. RMSE: 0.59 |\n",
            "| Epoch: 17 | Train Loss: 0.79 |Train MSE: 0.79 | Train RMSE: 0.89 |          Val. Loss: 1.08 |Val. MSE: 1.08 |Val. RMSE: 1.04 |          Test. Loss: 1.09 |Test. MSE: 1.09 |Test. RMSE: 1.04 |\n",
            "| Epoch: 18 | Train Loss: 0.79 |Train MSE: 0.79 | Train RMSE: 0.89 |          Val. Loss: 0.35 |Val. MSE: 0.35 |Val. RMSE: 0.59 |          Test. Loss: 0.34 |Test. MSE: 0.34 |Test. RMSE: 0.59 |\n",
            "| Epoch: 19 | Train Loss: 0.68 |Train MSE: 0.68 | Train RMSE: 0.83 |          Val. Loss: 0.39 |Val. MSE: 0.39 |Val. RMSE: 0.62 |          Test. Loss: 0.37 |Test. MSE: 0.37 |Test. RMSE: 0.61 |\n",
            "| Epoch: 20 | Train Loss: 0.66 |Train MSE: 0.66 | Train RMSE: 0.81 |          Val. Loss: 0.48 |Val. MSE: 0.48 |Val. RMSE: 0.69 |          Test. Loss: 0.48 |Test. MSE: 0.48 |Test. RMSE: 0.69 |\n",
            "| Epoch: 21 | Train Loss: 0.61 |Train MSE: 0.61 | Train RMSE: 0.78 |          Val. Loss: 0.50 |Val. MSE: 0.50 |Val. RMSE: 0.71 |          Test. Loss: 0.50 |Test. MSE: 0.50 |Test. RMSE: 0.71 |\n",
            "| Epoch: 22 | Train Loss: 0.60 |Train MSE: 0.60 | Train RMSE: 0.78 |          Val. Loss: 0.43 |Val. MSE: 0.43 |Val. RMSE: 0.66 |          Test. Loss: 0.42 |Test. MSE: 0.42 |Test. RMSE: 0.65 |\n",
            "| Epoch: 23 | Train Loss: 0.57 |Train MSE: 0.57 | Train RMSE: 0.75 |          Val. Loss: 0.35 |Val. MSE: 0.35 |Val. RMSE: 0.60 |          Test. Loss: 0.34 |Test. MSE: 0.34 |Test. RMSE: 0.59 |\n",
            "| Epoch: 24 | Train Loss: 0.58 |Train MSE: 0.58 | Train RMSE: 0.76 |          Val. Loss: 0.39 |Val. MSE: 0.39 |Val. RMSE: 0.62 |          Test. Loss: 0.39 |Test. MSE: 0.39 |Test. RMSE: 0.62 |\n",
            "| Epoch: 25 | Train Loss: 0.57 |Train MSE: 0.57 | Train RMSE: 0.76 |          Val. Loss: 0.59 |Val. MSE: 0.59 |Val. RMSE: 0.77 |          Test. Loss: 0.60 |Test. MSE: 0.60 |Test. RMSE: 0.77 |\n",
            "| Epoch: 26 | Train Loss: 0.56 |Train MSE: 0.56 | Train RMSE: 0.75 |          Val. Loss: 0.40 |Val. MSE: 0.40 |Val. RMSE: 0.63 |          Test. Loss: 0.40 |Test. MSE: 0.40 |Test. RMSE: 0.63 |\n",
            "| Epoch: 27 | Train Loss: 0.58 |Train MSE: 0.58 | Train RMSE: 0.76 |          Val. Loss: 0.53 |Val. MSE: 0.53 |Val. RMSE: 0.72 |          Test. Loss: 0.53 |Test. MSE: 0.53 |Test. RMSE: 0.73 |\n",
            "| Epoch: 28 | Train Loss: 0.51 |Train MSE: 0.51 | Train RMSE: 0.71 |          Val. Loss: 0.35 |Val. MSE: 0.35 |Val. RMSE: 0.59 |          Test. Loss: 0.34 |Test. MSE: 0.34 |Test. RMSE: 0.58 |\n",
            "| Epoch: 29 | Train Loss: 0.51 |Train MSE: 0.51 | Train RMSE: 0.71 |          Val. Loss: 0.40 |Val. MSE: 0.40 |Val. RMSE: 0.63 |          Test. Loss: 0.40 |Test. MSE: 0.40 |Test. RMSE: 0.63 |\n",
            "| Epoch: 30 | Train Loss: 0.50 |Train MSE: 0.50 | Train RMSE: 0.71 |          Val. Loss: 0.37 |Val. MSE: 0.37 |Val. RMSE: 0.61 |          Test. Loss: 0.36 |Test. MSE: 0.36 |Test. RMSE: 0.60 |\n",
            "| Epoch: 31 | Train Loss: 0.51 |Train MSE: 0.51 | Train RMSE: 0.71 |          Val. Loss: 0.62 |Val. MSE: 0.62 |Val. RMSE: 0.79 |          Test. Loss: 0.63 |Test. MSE: 0.63 |Test. RMSE: 0.79 |\n",
            "| Epoch: 32 | Train Loss: 0.50 |Train MSE: 0.50 | Train RMSE: 0.71 |          Val. Loss: 0.33 |Val. MSE: 0.33 |Val. RMSE: 0.58 |          Test. Loss: 0.33 |Test. MSE: 0.33 |Test. RMSE: 0.57 |\n",
            "| Epoch: 33 | Train Loss: 0.47 |Train MSE: 0.47 | Train RMSE: 0.69 |          Val. Loss: 0.38 |Val. MSE: 0.38 |Val. RMSE: 0.62 |          Test. Loss: 0.38 |Test. MSE: 0.38 |Test. RMSE: 0.62 |\n",
            "| Epoch: 34 | Train Loss: 0.47 |Train MSE: 0.47 | Train RMSE: 0.69 |          Val. Loss: 0.33 |Val. MSE: 0.33 |Val. RMSE: 0.58 |          Test. Loss: 0.33 |Test. MSE: 0.33 |Test. RMSE: 0.57 |\n",
            "| Epoch: 35 | Train Loss: 0.47 |Train MSE: 0.47 | Train RMSE: 0.68 |          Val. Loss: 0.52 |Val. MSE: 0.52 |Val. RMSE: 0.72 |          Test. Loss: 0.52 |Test. MSE: 0.52 |Test. RMSE: 0.72 |\n",
            "| Epoch: 36 | Train Loss: 0.46 |Train MSE: 0.46 | Train RMSE: 0.68 |          Val. Loss: 0.36 |Val. MSE: 0.36 |Val. RMSE: 0.60 |          Test. Loss: 0.36 |Test. MSE: 0.36 |Test. RMSE: 0.60 |\n",
            "| Epoch: 37 | Train Loss: 0.47 |Train MSE: 0.47 | Train RMSE: 0.68 |          Val. Loss: 0.33 |Val. MSE: 0.33 |Val. RMSE: 0.58 |          Test. Loss: 0.33 |Test. MSE: 0.33 |Test. RMSE: 0.57 |\n",
            "| Epoch: 38 | Train Loss: 0.44 |Train MSE: 0.44 | Train RMSE: 0.67 |          Val. Loss: 0.34 |Val. MSE: 0.34 |Val. RMSE: 0.58 |          Test. Loss: 0.33 |Test. MSE: 0.33 |Test. RMSE: 0.57 |\n",
            "| Epoch: 39 | Train Loss: 0.45 |Train MSE: 0.45 | Train RMSE: 0.67 |          Val. Loss: 0.33 |Val. MSE: 0.33 |Val. RMSE: 0.58 |          Test. Loss: 0.33 |Test. MSE: 0.33 |Test. RMSE: 0.57 |\n",
            "| Epoch: 40 | Train Loss: 0.45 |Train MSE: 0.45 | Train RMSE: 0.67 |          Val. Loss: 0.34 |Val. MSE: 0.34 |Val. RMSE: 0.58 |          Test. Loss: 0.33 |Test. MSE: 0.33 |Test. RMSE: 0.58 |\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hFkXsS5w5Qyk"
      },
      "source": [
        "#### Generate CSV output for competition"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rds8pY5N4ORv"
      },
      "source": [
        "# this part of code used to generate the competion out\r\n",
        "import csv\r\n",
        "test_df_id = pd.read_csv(test_file)\r\n",
        "test_df_id = test_df_id[['id']]\r\n",
        "test_df_id = test_df_id.values.tolist()\r\n",
        "\r\n",
        "with open('task-1-output.csv', 'w', newline='') as outcsv:\r\n",
        "    writer = csv.DictWriter(outcsv, fieldnames = ['id', 'pred'])\r\n",
        "    writer.writeheader()\r\n",
        "    for i in range(len(test_df_id)):\r\n",
        "        writer.writerow({'id': str(test_df_id[i][0]), 'pred': str(predicted[i])})\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZAVQcBstrsr4"
      },
      "source": [
        "Hyper-parameter Tuning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q_EOhvCRqihg"
      },
      "source": [
        "'''\r\n",
        "# Here are the example codes for our hyper-parameter tuning.\r\n",
        "# To run this, you also needs to modify the return part of train()\r\n",
        "# Approach 1\r\n",
        "torch.cuda.empty_cache()\r\n",
        "config = Config\r\n",
        "train_file = '/content/NLP_CW1/Matrixposer/data/train.csv'\r\n",
        "test_file = '/content/NLP_CW1/Matrixposer/data/test.csv'\r\n",
        "val_file = '/content/NLP_CW1/Matrixposer/data/dev.csv'\r\n",
        "\r\n",
        "pre_output = []\r\n",
        "no_pre_output = []\r\n",
        "\r\n",
        "# Number of epochs\r\n",
        "num_epochs = [40]\r\n",
        "#num_epochs = np.arange(10,80,10)\r\n",
        "num_lr = [0.013]\r\n",
        "#num_lr = np.arange(0.0001,0.1,0.001)\r\n",
        "dropouts = np.arange(0,1,0.02)\r\n",
        "\r\n",
        "config.pre_trained = True\r\n",
        "for epochs in num_epochs:\r\n",
        "    for lr in num_lr:\r\n",
        "        for dropout_ in dropouts:\r\n",
        "            config.dropout = dropout_ \r\n",
        "            config.lr = lr\r\n",
        "            dataset = Dataset(config)\r\n",
        "            dataset.load_data(train_file, test_file, val_file, config.pre_trained)\r\n",
        "            model = Matposer(config, dataset.vocab, config.pre_trained)\r\n",
        "            if torch.cuda.is_available():\r\n",
        "                model.cuda()\r\n",
        "            optimizer = optim.Adam(model.parameters(), lr=config.lr)\r\n",
        "            criterion = nn.MSELoss()\r\n",
        "            criterion = criterion.cuda()\r\n",
        "            hype_epochs, hype_lr, hype_train, hype_train_loss, hype_valid, hype_valid_loss, hype_test,hype_test_loss,hype_drop = train(dataset.train_iterator, dataset.val_iterator,model, epochs)\r\n",
        "            pre_output.append([hype_epochs, hype_lr, hype_train, hype_train_loss, hype_valid, hype_valid_loss, hype_test,hype_test_loss,hype_drop])\r\n",
        "\r\n",
        "# Approach 2\r\n",
        "config.pre_trained = False\r\n",
        "num_epochs = [40]\r\n",
        "#num_epochs = np.arange(10,80,10)\r\n",
        "num_lr = [0.0001]\r\n",
        "#num_lr = np.arange(0.0001,0.1,0.001)\r\n",
        "dropouts = np.arange(0,1,0.02)\r\n",
        "for epochs in num_epochs:\r\n",
        "    for lr in num_lr:\r\n",
        "        for dropout_ in dropouts:\r\n",
        "            config.dropout = dropout_ \r\n",
        "            config.lr = lr\r\n",
        "            dataset = Dataset(config)\r\n",
        "            dataset.load_data(train_file, test_file, val_file, config.pre_trained)\r\n",
        "            model = Matposer(config, dataset.vocab, config.pre_trained)\r\n",
        "            if torch.cuda.is_available():\r\n",
        "                model.cuda()\r\n",
        "            optimizer = optim.Adam(model.parameters(), lr=config.lr)\r\n",
        "            criterion = nn.MSELoss()\r\n",
        "            criterion = criterion.cuda()\r\n",
        "            hype_epochs, hype_lr, hype_train, hype_train_loss, hype_valid, hype_valid_loss, hype_test,hype_test_loss,hype_drop = train(dataset.train_iterator, dataset.val_iterator,model, epochs)\r\n",
        "            no_pre_output.append([hype_epochs, hype_lr, hype_train, hype_train_loss, hype_valid, hype_valid_loss, hype_test,hype_test_loss,hype_drop])\r\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mMNZd9xysJ2y"
      },
      "source": [
        "'''\r\n",
        "# Here are the example codes for generate hyper-parameter graph.\r\n",
        "def plot_test(input_data, pre):\r\n",
        "    plt.figure()\r\n",
        "    for epoch_ in num_epochs:\r\n",
        "        drop_list,epoch_list, lr_list, train_RMSE, valid_RMSE, test_RMSE, tran_loss, valid_loss, test_loss = [],[],[],[],[],[],[],[],[]\r\n",
        "        for item in input_data:\r\n",
        "            if epoch_ == item[0]: \r\n",
        "                lr_list.append(item[1])\r\n",
        "                valid_RMSE.append(item[4])\r\n",
        "                drop_list.append(item[8]) \r\n",
        "        plt.plot(drop_list, valid_RMSE, label = 'Valid RMSE epoch = {}'.format(epoch_))\r\n",
        "    plt.xlabel('Dropout')\r\n",
        "    plt.ylabel('RMSE')\r\n",
        "    if pre:\r\n",
        "        plt.title('Dropout V.S RMSE with pre-trained')\r\n",
        "    else:\r\n",
        "        plt.title('Dropout V.S RMSE with no pre-trained')\r\n",
        "        plt.ylim((0.4,1.5))\r\n",
        "    plt.legend(loc='upper left')\r\n",
        "    if pre:\r\n",
        "        plt.savefig('/content/hype_drop_testRmse_pre_.jpg')\r\n",
        "    else:\r\n",
        "        plt.savefig('/content/hype_drop_testRmse_nopre.jpg')\r\n",
        "    plt.show()\r\n",
        "\r\n",
        "    plt.figure()\r\n",
        "    for epoch_ in num_epochs:\r\n",
        "        drop_list, epoch_list, lr_list, train_RMSE, valid_RMSE, test_RMSE, tran_loss, valid_loss, test_loss = [],[],[],[],[],[],[],[],[]\r\n",
        "        for item in input_data:\r\n",
        "            if epoch_ == item[0]:\r\n",
        "                lr_list.append(item[1])\r\n",
        "                valid_loss.append(item[5])\r\n",
        "                drop_list.append(item[8])\r\n",
        "        plt.plot(drop_list, valid_loss, label = 'valid Loss epoch = {}'.format(epoch_))\r\n",
        "    plt.xlabel('Dropout')\r\n",
        "    plt.ylabel('Loss')\r\n",
        "\r\n",
        "    if pre:\r\n",
        "        plt.title('Dropout V.S Loss with pre-trained')\r\n",
        "    else:\r\n",
        "        plt.ylim((0.4,1.5))\r\n",
        "        plt.title('Dropout V.S Loss with no pre-trained')\r\n",
        "\r\n",
        "    plt.legend(loc='upper left')\r\n",
        "    if pre:\r\n",
        "        plt.savefig('/content/hype_drop_testLoss_pre.jpg')\r\n",
        "    else:\r\n",
        "        plt.savefig('/content/hype_drop_testLoss_nopre.jpg')\r\n",
        "    plt.show()\r\n",
        "\r\n",
        "plot_test(pre_output,True)\r\n",
        "plot_test(no_pre_output,False)\r\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}